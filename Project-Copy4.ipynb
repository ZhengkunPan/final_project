{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86967501",
   "metadata": {},
   "source": [
    "# Part 1: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f98c64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import geopandas as gpd\n",
    "import bs4\n",
    "import re\n",
    "import requests\n",
    "import os\n",
    "import pyarrow.parquet as pq\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "import folium.plugins\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import pytest\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84125c4b",
   "metadata": {},
   "source": [
    "### Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6b837b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>passenger_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24238194</th>\n",
       "      <td>2015-05-07 19:52:06.0000003</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2015-05-07 19:52:06 UTC</td>\n",
       "      <td>-73.999817</td>\n",
       "      <td>40.738354</td>\n",
       "      <td>-73.999512</td>\n",
       "      <td>40.723217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27835199</th>\n",
       "      <td>2009-07-17 20:04:56.0000002</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2009-07-17 20:04:56 UTC</td>\n",
       "      <td>-73.994355</td>\n",
       "      <td>40.728225</td>\n",
       "      <td>-73.994710</td>\n",
       "      <td>40.750325</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44984355</th>\n",
       "      <td>2009-08-24 21:45:00.00000061</td>\n",
       "      <td>12.9</td>\n",
       "      <td>2009-08-24 21:45:00 UTC</td>\n",
       "      <td>-74.005043</td>\n",
       "      <td>40.740770</td>\n",
       "      <td>-73.962565</td>\n",
       "      <td>40.772647</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25894730</th>\n",
       "      <td>2009-06-26 08:22:21.0000001</td>\n",
       "      <td>5.3</td>\n",
       "      <td>2009-06-26 08:22:21 UTC</td>\n",
       "      <td>-73.976124</td>\n",
       "      <td>40.790844</td>\n",
       "      <td>-73.965316</td>\n",
       "      <td>40.803349</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17610152</th>\n",
       "      <td>2014-08-28 17:47:00.000000188</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2014-08-28 17:47:00 UTC</td>\n",
       "      <td>-73.925023</td>\n",
       "      <td>40.744085</td>\n",
       "      <td>-73.973082</td>\n",
       "      <td>40.761247</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    key  fare_amount          pickup_datetime   \n",
       "24238194    2015-05-07 19:52:06.0000003          7.5  2015-05-07 19:52:06 UTC  \\\n",
       "27835199    2009-07-17 20:04:56.0000002          7.7  2009-07-17 20:04:56 UTC   \n",
       "44984355   2009-08-24 21:45:00.00000061         12.9  2009-08-24 21:45:00 UTC   \n",
       "25894730    2009-06-26 08:22:21.0000001          5.3  2009-06-26 08:22:21 UTC   \n",
       "17610152  2014-08-28 17:47:00.000000188         16.0  2014-08-28 17:47:00 UTC   \n",
       "\n",
       "          pickup_longitude  pickup_latitude  dropoff_longitude   \n",
       "24238194        -73.999817        40.738354         -73.999512  \\\n",
       "27835199        -73.994355        40.728225         -73.994710   \n",
       "44984355        -74.005043        40.740770         -73.962565   \n",
       "25894730        -73.976124        40.790844         -73.965316   \n",
       "17610152        -73.925023        40.744085         -73.973082   \n",
       "\n",
       "          dropoff_latitude  passenger_count  \n",
       "24238194         40.723217                1  \n",
       "27835199         40.750325                1  \n",
       "44984355         40.772647                1  \n",
       "25894730         40.803349                3  \n",
       "17610152         40.761247                5  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber_rides_sample = pd.read_csv(\"uber_rides_sample.csv\", index_col = 0)\n",
    "uber_rides_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a55db75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 200000 entries, 24238194 to 11951496\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   key                200000 non-null  object \n",
      " 1   fare_amount        200000 non-null  float64\n",
      " 2   pickup_datetime    200000 non-null  object \n",
      " 3   pickup_longitude   200000 non-null  float64\n",
      " 4   pickup_latitude    200000 non-null  float64\n",
      " 5   dropoff_longitude  199999 non-null  float64\n",
      " 6   dropoff_latitude   199999 non-null  float64\n",
      " 7   passenger_count    200000 non-null  int64  \n",
      "dtypes: float64(5), int64(1), object(2)\n",
      "memory usage: 13.7+ MB\n"
     ]
    }
   ],
   "source": [
    "uber_rides_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797bb68a",
   "metadata": {},
   "source": [
    "### Download Yellow Taxi Data and get a sample dataset with its size close to Uber Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db4d48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = set()\n",
    "# for filename in os.listdir(\"./yellow_taxi\"):\n",
    "#     if filename.endswith(\".parquet\"):\n",
    "#         existing_columns = set(pq.ParquetFile(\"./yellow_taxi/\"+filename).schema.names)\n",
    "#         result = result.union(existing_columns)\n",
    "\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f27339d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Yellow Taxi Parquet files\n",
    "def download_yellow_taxi_parquet_files():\n",
    "    \"\"\"\n",
    "    Downloads Yellow Taxi trip record data files in Parquet format from the NYC Taxi and Limousine Commission website.\n",
    "    \n",
    "    This function sends a GET request to the webpage containing the links to the Yellow Taxi trip record data files,\n",
    "    parses the HTML response using BeautifulSoup, and downloads any files matching the pattern \"yellow_tripdata_YYYY-MM.parquet\",\n",
    "    where YYYY-MM is a year-month combination from 2009-01 to 2015-12.\n",
    "    \n",
    "    The downloaded files are saved in a directory called 'yellow_taxi' in the current working directory.\n",
    "    \n",
    "    Raises:\n",
    "        OSError: If the directory 'yellow_taxi' already exists and is not writable, or if there is a problem writing to any of the downloaded files.\n",
    "        requests.exceptions.RequestException: If there is a problem sending the GET request to the webpage containing the links to the data files.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = requests.get(\"https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page\")\n",
    "    soup = bs4.BeautifulSoup(response.content, 'html.parser')\n",
    "    yellow_records = soup.find_all(\"a\", attrs={\"title\": \"Yellow Taxi Trip Records\"})\n",
    "    if not os.path.exists(\"./yellow_taxi\"):\n",
    "        os.makedirs(\"./yellow_taxi\")\n",
    "    \n",
    "    for record in yellow_records:\n",
    "        pattern = r'yellow_tripdata_(2009|201[0-5])-\\d{2}\\.'\n",
    "        link = record[\"href\"]\n",
    "        if re.search(pattern, link):\n",
    "            filename = os.path.join(\"yellow_taxi\", link.split(\"/\")[-1])\n",
    "            response = requests.get(link)\n",
    "            \n",
    "            with open(filename, \"wb\") as f:\n",
    "                f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a65e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComment the line below if no need to download the Yellow Taxi Parquet files.\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comment the line below if no need to download the Yellow Taxi Parquet files.\n",
    "\"\"\"\n",
    "\n",
    "# download_yellow_taxi_parquet_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65f97de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Yellow Taxi Data sample\n",
    "def generate_yellow_taxi_df():\n",
    "    \"\"\"\n",
    "    Reads Parquet files containing Yellow Taxi trip data, selects a subset of columns, and outputs a CSV file with a random sample of 3000 rides.\n",
    "    \n",
    "    This function reads Parquet files in the './yellow_taxi' directory, selects a subset of columns from the schema\n",
    "    (specified in the 'columns_to_select' list), and reads the data into a Pandas DataFrame. A random sample of 3000\n",
    "    rows is taken from each file, and the resulting DataFrames are concatenated into a single DataFrame. Finally, this\n",
    "    DataFrame is saved to a CSV file called 'yellow_taxi_ride_sample.csv' in the current working directory.\n",
    "    \n",
    "    Raises:\n",
    "        pyarrow.lib.ArrowInvalid: If there is an error reading the Parquet file schema.\n",
    "        pyarrow.lib.ArrowIOError: If there is an error reading the Parquet file.\n",
    "    \"\"\"   \n",
    "    \n",
    "    columns_to_select = ['DOLocationID',\n",
    "                         'End_Lat',\n",
    "                         'End_Lon',\n",
    "                         'PULocationID',\n",
    "                         'Passenger_Count',\n",
    "                         'Start_Lat',\n",
    "                         'Start_Lon',\n",
    "                         'Total_Amt',\n",
    "                         'Trip_Distance',\n",
    "                         'Trip_Dropoff_DateTime',\n",
    "                         'Trip_Pickup_DateTime',\n",
    "                         'dropoff_datetime',\n",
    "                         'dropoff_latitude',\n",
    "                         'dropoff_longitude',\n",
    "                         'passenger_count',\n",
    "                         'pickup_datetime',\n",
    "                         'pickup_latitude',\n",
    "                         'pickup_longitude',\n",
    "                         'total_amount',\n",
    "                         'tpep_dropoff_datetime',\n",
    "                         'tpep_pickup_datetime',\n",
    "                         'trip_distance',\n",
    "                         'Tip_Amt',\n",
    "                         'tip_amount']\n",
    "\n",
    "    directory = \"./yellow_taxi\"\n",
    "    yellow_taix_df = pd.DataFrame()\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".parquet\"):\n",
    "            existing_columns = set(pq.ParquetFile(\"./yellow_taxi/\"+filename).schema.names)\n",
    "            columns_to_read = list(set(columns_to_select) & existing_columns)\n",
    "            table = pq.read_table(\"./yellow_taxi/\"+filename, columns=columns_to_read)\n",
    "            df = table.to_pandas()\n",
    "            random_subset = df.sample(n=3000)\n",
    "            yellow_taix_df = pd.concat([yellow_taix_df, random_subset], ignore_index=True)\n",
    "    yellow_taix_df.to_csv(\"yellow_taxi_ride_sample.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b9e0955",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nComment the line below if the Yellow Taxi Data sample already generated.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Comment the line below if the Yellow Taxi Data sample already generated.\n",
    "\"\"\"\n",
    "\n",
    "# generate_yellow_taxi_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ed42c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>total_amount</th>\n",
       "      <th>DOLocationID</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>Passenger_Count</th>\n",
       "      <th>Tip_Amt</th>\n",
       "      <th>...</th>\n",
       "      <th>Trip_Distance</th>\n",
       "      <th>End_Lat</th>\n",
       "      <th>Trip_Pickup_DateTime</th>\n",
       "      <th>End_Lon</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>dropoff_datetime</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>pickup_datetime</th>\n",
       "      <th>pickup_longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.5</td>\n",
       "      <td>1.50</td>\n",
       "      <td>2011-07-20 06:58:52</td>\n",
       "      <td>170.0</td>\n",
       "      <td>8.10</td>\n",
       "      <td>230.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-07-20 06:52:08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.9</td>\n",
       "      <td>1.26</td>\n",
       "      <td>2011-07-09 00:53:28</td>\n",
       "      <td>48.0</td>\n",
       "      <td>7.56</td>\n",
       "      <td>48.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-07-09 00:48:14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2011-07-14 10:36:55</td>\n",
       "      <td>140.0</td>\n",
       "      <td>9.00</td>\n",
       "      <td>142.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2011-07-14 10:25:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2011-07-02 14:06:27</td>\n",
       "      <td>43.0</td>\n",
       "      <td>6.60</td>\n",
       "      <td>161.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2011-07-02 13:59:48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.5</td>\n",
       "      <td>5.00</td>\n",
       "      <td>2011-07-16 21:26:28</td>\n",
       "      <td>138.0</td>\n",
       "      <td>36.10</td>\n",
       "      <td>231.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2011-07-16 20:55:22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   trip_distance  tip_amount tpep_dropoff_datetime  PULocationID   \n",
       "0            1.5        1.50   2011-07-20 06:58:52         170.0  \\\n",
       "1            0.9        1.26   2011-07-09 00:53:28          48.0   \n",
       "2            1.9        0.00   2011-07-14 10:36:55         140.0   \n",
       "3            1.5        0.00   2011-07-02 14:06:27          43.0   \n",
       "4           12.5        5.00   2011-07-16 21:26:28         138.0   \n",
       "\n",
       "   total_amount  DOLocationID  passenger_count tpep_pickup_datetime   \n",
       "0          8.10         230.0              1.0  2011-07-20 06:52:08  \\\n",
       "1          7.56          48.0              2.0  2011-07-09 00:48:14   \n",
       "2          9.00         142.0              2.0  2011-07-14 10:25:30   \n",
       "3          6.60         161.0              1.0  2011-07-02 13:59:48   \n",
       "4         36.10         231.0              3.0  2011-07-16 20:55:22   \n",
       "\n",
       "   Passenger_Count  Tip_Amt  ... Trip_Distance  End_Lat  Trip_Pickup_DateTime   \n",
       "0              NaN      NaN  ...           NaN      NaN                   NaN  \\\n",
       "1              NaN      NaN  ...           NaN      NaN                   NaN   \n",
       "2              NaN      NaN  ...           NaN      NaN                   NaN   \n",
       "3              NaN      NaN  ...           NaN      NaN                   NaN   \n",
       "4              NaN      NaN  ...           NaN      NaN                   NaN   \n",
       "\n",
       "   End_Lon  dropoff_latitude  dropoff_datetime dropoff_longitude   \n",
       "0      NaN               NaN               NaN               NaN  \\\n",
       "1      NaN               NaN               NaN               NaN   \n",
       "2      NaN               NaN               NaN               NaN   \n",
       "3      NaN               NaN               NaN               NaN   \n",
       "4      NaN               NaN               NaN               NaN   \n",
       "\n",
       "   pickup_latitude  pickup_datetime pickup_longitude  \n",
       "0              NaN              NaN              NaN  \n",
       "1              NaN              NaN              NaN  \n",
       "2              NaN              NaN              NaN  \n",
       "3              NaN              NaN              NaN  \n",
       "4              NaN              NaN              NaN  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yellow_taxi_ride_sample = pd.read_csv(\"yellow_taxi_ride_sample.csv\")\n",
    "yellow_taxi_ride_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14151f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 252000 entries, 0 to 251999\n",
      "Data columns (total 24 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   trip_distance          216000 non-null  float64\n",
      " 1   tip_amount             216000 non-null  float64\n",
      " 2   tpep_dropoff_datetime  180000 non-null  object \n",
      " 3   PULocationID           180000 non-null  float64\n",
      " 4   total_amount           216000 non-null  float64\n",
      " 5   DOLocationID           180000 non-null  float64\n",
      " 6   passenger_count        216000 non-null  float64\n",
      " 7   tpep_pickup_datetime   180000 non-null  object \n",
      " 8   Passenger_Count        36000 non-null   float64\n",
      " 9   Tip_Amt                36000 non-null   float64\n",
      " 10  Trip_Dropoff_DateTime  36000 non-null   object \n",
      " 11  Start_Lat              36000 non-null   float64\n",
      " 12  Start_Lon              36000 non-null   float64\n",
      " 13  Total_Amt              36000 non-null   float64\n",
      " 14  Trip_Distance          36000 non-null   float64\n",
      " 15  End_Lat                36000 non-null   float64\n",
      " 16  Trip_Pickup_DateTime   36000 non-null   object \n",
      " 17  End_Lon                36000 non-null   float64\n",
      " 18  dropoff_latitude       36000 non-null   float64\n",
      " 19  dropoff_datetime       36000 non-null   object \n",
      " 20  dropoff_longitude      36000 non-null   float64\n",
      " 21  pickup_latitude        36000 non-null   float64\n",
      " 22  pickup_datetime        36000 non-null   object \n",
      " 23  pickup_longitude       36000 non-null   float64\n",
      "dtypes: float64(18), object(6)\n",
      "memory usage: 46.1+ MB\n"
     ]
    }
   ],
   "source": [
    "yellow_taxi_ride_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296b4b89",
   "metadata": {},
   "source": [
    "### Data Preprocessing - Yellow Taxi Data\n",
    "#### Combine columns with the same thing BUT have different column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93ef96b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_datetime(df):\n",
    "    \"\"\"\n",
    "    Convert specified columns in a pandas DataFrame to datetime format.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): A DataFrame with columns to be converted.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A DataFrame with converted datetime columns.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    df[\"tpep_pickup_datetime\"] = pd.to_datetime(df[\"tpep_pickup_datetime\"])\n",
    "    df[\"Trip_Pickup_DateTime\"] = pd.to_datetime(df[\"Trip_Pickup_DateTime\"])\n",
    "    df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\n",
    "\n",
    "    df[\"tpep_dropoff_datetime\"] = pd.to_datetime(df[\"tpep_dropoff_datetime\"])\n",
    "    df[\"Trip_Dropoff_DateTime\"] = pd.to_datetime(df[\"Trip_Dropoff_DateTime\"])\n",
    "    df[\"dropoff_datetime\"] = pd.to_datetime(df[\"dropoff_datetime\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "21aca131",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 252000 entries, 0 to 251999\n",
      "Data columns (total 24 columns):\n",
      " #   Column                 Non-Null Count   Dtype         \n",
      "---  ------                 --------------   -----         \n",
      " 0   trip_distance          216000 non-null  float64       \n",
      " 1   tip_amount             216000 non-null  float64       \n",
      " 2   tpep_dropoff_datetime  180000 non-null  datetime64[ns]\n",
      " 3   PULocationID           180000 non-null  float64       \n",
      " 4   total_amount           216000 non-null  float64       \n",
      " 5   DOLocationID           180000 non-null  float64       \n",
      " 6   passenger_count        216000 non-null  float64       \n",
      " 7   tpep_pickup_datetime   180000 non-null  datetime64[ns]\n",
      " 8   Passenger_Count        36000 non-null   float64       \n",
      " 9   Tip_Amt                36000 non-null   float64       \n",
      " 10  Trip_Dropoff_DateTime  36000 non-null   datetime64[ns]\n",
      " 11  Start_Lat              36000 non-null   float64       \n",
      " 12  Start_Lon              36000 non-null   float64       \n",
      " 13  Total_Amt              36000 non-null   float64       \n",
      " 14  Trip_Distance          36000 non-null   float64       \n",
      " 15  End_Lat                36000 non-null   float64       \n",
      " 16  Trip_Pickup_DateTime   36000 non-null   datetime64[ns]\n",
      " 17  End_Lon                36000 non-null   float64       \n",
      " 18  dropoff_latitude       36000 non-null   float64       \n",
      " 19  dropoff_datetime       36000 non-null   datetime64[ns]\n",
      " 20  dropoff_longitude      36000 non-null   float64       \n",
      " 21  pickup_latitude        36000 non-null   float64       \n",
      " 22  pickup_datetime        36000 non-null   datetime64[ns]\n",
      " 23  pickup_longitude       36000 non-null   float64       \n",
      "dtypes: datetime64[ns](6), float64(18)\n",
      "memory usage: 46.1 MB\n"
     ]
    }
   ],
   "source": [
    "yellow_taxi_ride_sample = convert_to_datetime(yellow_taxi_ride_sample)\n",
    "yellow_taxi_ride_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cdc9276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert yellow_taxi_ride_sample.shape[0] == 252000\n",
    "assert yellow_taxi_ride_sample.shape[1] == 24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed5064f7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 161)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:161\u001b[0;36m\u001b[0m\n\u001b[0;31m    if pd.notna(row['passenger_count']):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "def impute_pickup_datetime(row):\n",
    "    \"\"\"\n",
    "    Impute missing pickup datetime.\n",
    "\n",
    "    If `tpep_pickup_datetime` is not NaN, return it.\n",
    "    Else if `pickup_datetime` is not NaN, return it.\n",
    "    Else return `Trip_Pickup_DateTime`.\n",
    "\n",
    "    Args:\n",
    "        row: a pandas Series containing information about a taxi ride.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Timestamp object representing the pickup datetime.\n",
    "    \"\"\"\n",
    "    if pd.notna(row['tpep_pickup_datetime']):\n",
    "        return row['tpep_pickup_datetime']\n",
    "    elif pd.notna(row['pickup_datetime']):\n",
    "        return row['pickup_datetime']\n",
    "    else:\n",
    "        return row['Trip_Pickup_DateTime']\n",
    "    \n",
    "def impute_dropoff_datetime(row):\n",
    "    \"\"\"\n",
    "    Impute missing dropoff datetime.\n",
    "\n",
    "    If `tpep_dropoff_datetime` is not NaN, return it.\n",
    "    Else if `dropoff_datetime` is not NaN, return it.\n",
    "    Else return `Trip_Dropoff_DateTime`.\n",
    "\n",
    "    Args:\n",
    "        row: a pandas Series containing information about a taxi ride.\n",
    "\n",
    "    Returns:\n",
    "        A pandas Timestamp object representing the dropoff datetime.\n",
    "    \"\"\"\n",
    "    if pd.notna(row['tpep_dropoff_datetime']):\n",
    "        return row['tpep_dropoff_datetime']\n",
    "    elif pd.notna(row['dropoff_datetime']):\n",
    "        return row['dropoff_datetime']\n",
    "    else:\n",
    "        return row['Trip_Dropoff_DateTime']\n",
    "    \n",
    "def impute_trip_distance(row):\n",
    "    \"\"\"\n",
    "    Impute missing trip distance.\n",
    "\n",
    "    If `trip_distance` is not NaN, return it.\n",
    "    Else return `Trip_Distance`.\n",
    "\n",
    "    Args:\n",
    "        row: a pandas Series containing information about a taxi ride.\n",
    "\n",
    "    Returns:\n",
    "        A float representing the trip distance.\n",
    "    \"\"\"\n",
    "    if pd.notna(row['trip_distance']):\n",
    "        return row['trip_distance']\n",
    "    else:\n",
    "        return row['Trip_Distance']\n",
    "    \n",
    "def impute_total_amount(row):\n",
    "    \"\"\"\n",
    "    Impute missing total amount.\n",
    "\n",
    "    If `Total_Amt` is not NaN, return it.\n",
    "    Else return `total_amount`.\n",
    "\n",
    "    Args:\n",
    "        row: a pandas Series containing information about a taxi ride.\n",
    "\n",
    "    Returns:\n",
    "        A float representing the total amount.\n",
    "    \"\"\" \n",
    "    if pd.notna(row['Total_Amt']):\n",
    "        return row['Total_Amt']\n",
    "    else:\n",
    "        return row['total_amount']\n",
    "    \n",
    "def impute_pickup_lon(row):\n",
    "    \"\"\"\n",
    "    Impute missing pickup longitude.\n",
    "\n",
    "    If `pickup_longitude` is not NaN, return it.\n",
    "    Else return `Start_Lon`.\n",
    "\n",
    "    Args:\n",
    "        row: a pandas Series containing information about a taxi ride.\n",
    "\n",
    "    Returns:\n",
    "        A float representing the pickup longitude.\n",
    "    \"\"\"\n",
    "    if pd.notna(row['pickup_longitude']):\n",
    "        return row['pickup_longitude']\n",
    "    else:\n",
    "        return row['Start_Lon']\n",
    "    \n",
    "def impute_pickup_lat(row):\n",
    "    \"\"\"\n",
    "    Impute missing pickup latitude.\n",
    "\n",
    "    If `pickup_latitude` is not NaN, return it.\n",
    "    Else return `Start_Lat`.\n",
    "\n",
    "    Args:\n",
    "        row: a pandas Series containing information about a taxi ride.\n",
    "\n",
    "    Returns:\n",
    "        A float representing the pickup latitude.\n",
    "    \"\"\"\n",
    "    if pd.notna(row['pickup_latitude']):\n",
    "        return row['pickup_latitude']\n",
    "    else:\n",
    "        return row['Start_Lat']\n",
    "\n",
    "def impute_dropoff_lon(row):\n",
    "    \"\"\"Impute missing dropoff longitude.\n",
    "\n",
    "    If `dropoff_longitude` is not NaN, return it.\n",
    "    Else return `End_Lon`.\n",
    "\n",
    "    Args:\n",
    "        row: a pandas Series containing information about a taxi ride.\n",
    "\n",
    "    Returns:\n",
    "        A float representing the dropoff longitude.\n",
    "    \"\"\"\n",
    "    if pd.notna(row['dropoff_longitude']):\n",
    "        return row['dropoff_longitude']\n",
    "    else:\n",
    "        return row['End_Lon']\n",
    "\n",
    "def impute_dropoff_lat(row):\n",
    "    \"\"\"Impute missing dropoff latitude.\n",
    "\n",
    "    If `dropoff_latitude` is not NaN, return it.\n",
    "    Else return `End_Lat`.\n",
    "\n",
    "    Args:\n",
    "        row: a pandas Series containing information about a taxi ride.\n",
    "\n",
    "    Returns:\n",
    "        A float representing the dropoff latitude.\n",
    "    \"\"\"\n",
    "    if pd.notna(row['dropoff_latitude']):\n",
    "        return row['dropoff_latitude']\n",
    "    else:\n",
    "        return row['End_Lat']\n",
    "    \n",
    "def impute_passenger_count(row):\n",
    "     \"\"\"Impute missing passenger count.\n",
    "\n",
    "    If `passenger_count` is not NaN, return it.\n",
    "    Else return `Passenger_Count`.\n",
    "\n",
    "    Args:\n",
    "        row: a pandas Series containing information about a taxi ride.\n",
    "\n",
    "    Returns:\n",
    "        An integer representing the number of passengers.\n",
    "    \"\"\"\n",
    "    if pd.notna(row['passenger_count']):\n",
    "        return row['passenger_count']\n",
    "    else:\n",
    "        return row['Passenger_Count']\n",
    "    \n",
    "def impute_tip(row):\n",
    "    \"\"\"Impute missing tip.\n",
    "\n",
    "    If `tip_amount` is not NaN, return it.\n",
    "    Else return `Tip_Amt`.\n",
    "\n",
    "    Args:\n",
    "        row: a pandas Series containing information about a taxi ride.\n",
    "\n",
    "    Returns:\n",
    "        A float representing the tip amount.\n",
    "    \"\"\"\n",
    "    if pd.notna(row['tip_amount']):\n",
    "        return row['tip_amount']\n",
    "    else:\n",
    "        return row['Tip_Amt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi_ride_sample['pickup_datetime'] = yellow_taxi_ride_sample.apply(impute_pickup_datetime, axis=1)\n",
    "yellow_taxi_ride_sample['dropoff_datetime'] = yellow_taxi_ride_sample.apply(impute_dropoff_datetime, axis=1)\n",
    "yellow_taxi_ride_sample['trip_distance'] = yellow_taxi_ride_sample.apply(impute_trip_distance, axis=1)\n",
    "yellow_taxi_ride_sample['total_amount'] = yellow_taxi_ride_sample.apply(impute_total_amount, axis=1)\n",
    "yellow_taxi_ride_sample['pickup_longitude'] = yellow_taxi_ride_sample.apply(impute_pickup_lon, axis=1)\n",
    "yellow_taxi_ride_sample['pickup_latitude'] = yellow_taxi_ride_sample.apply(impute_pickup_lat, axis=1)\n",
    "yellow_taxi_ride_sample['dropoff_longitude'] = yellow_taxi_ride_sample.apply(impute_dropoff_lon, axis=1)\n",
    "yellow_taxi_ride_sample['dropoff_latitude'] = yellow_taxi_ride_sample.apply(impute_dropoff_lat, axis=1)\n",
    "yellow_taxi_ride_sample['passenger_count'] = yellow_taxi_ride_sample.apply(impute_passenger_count, axis=1)\n",
    "yellow_taxi_ride_sample['passenger_count'] = yellow_taxi_ride_sample['passenger_count'].astype(int)\n",
    "yellow_taxi_ride_sample['tip_amount'] = yellow_taxi_ride_sample.apply(impute_tip, axis=1)\n",
    "yellow_taxi_ride_sample = yellow_taxi_ride_sample.drop(columns=['tpep_pickup_datetime', 'Trip_Pickup_DateTime',\\\n",
    "                                                                'tpep_dropoff_datetime', 'Trip_Dropoff_DateTime',\\\n",
    "                                                                'Trip_Distance', 'Total_Amt',\\\n",
    "                                                                'Start_Lon', 'Start_Lat',\\\n",
    "                                                                'End_Lon', 'End_Lat', 'Passenger_Count', 'Tip_Amt'])\n",
    "yellow_taxi_ride_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13888669",
   "metadata": {},
   "source": [
    "#### Deal with  `longitude` , `latitude`, and `Location ID` related problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31112fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows_PUlon = yellow_taxi_ride_sample[yellow_taxi_ride_sample['pickup_longitude'].isnull() & yellow_taxi_ride_sample['PULocationID'].notnull()]\n",
    "nan_rows_PUlat = yellow_taxi_ride_sample[yellow_taxi_ride_sample['pickup_latitude'].isnull() & yellow_taxi_ride_sample['PULocationID'].notnull()]\n",
    "nan_rows_DOlon = yellow_taxi_ride_sample[yellow_taxi_ride_sample['dropoff_longitude'].isnull() & yellow_taxi_ride_sample['DOLocationID'].notnull()]\n",
    "nan_rows_DOlat = yellow_taxi_ride_sample[yellow_taxi_ride_sample['dropoff_latitude'].isnull() & yellow_taxi_ride_sample['DOLocationID'].notnull()]\n",
    "nan_rows_lon_lat = pd.concat([nan_rows_PUlon, nan_rows_PUlat, nan_rows_DOlon, nan_rows_DOlat], axis=1)\n",
    "nan_rows_lon_lat = nan_rows_lon_lat.loc[:, ~nan_rows_lon_lat.columns.duplicated()]\n",
    "nan_rows_lon_lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a184f3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_rows = yellow_taxi_ride_sample.loc[~yellow_taxi_ride_sample.index.isin(nan_rows_lon_lat.index)]\n",
    "rows_lon_lat_exist = yellow_taxi_ride_sample[yellow_taxi_ride_sample['pickup_longitude'].notnull() & yellow_taxi_ride_sample['pickup_latitude'].notnull()]\n",
    "print(f\"Check 'rows_lon_lat_exist' and 'nan_rows_lon_lat' forms a partition of 'yellow_taxi_ride_sample': {(rest_rows.copy().drop(['PULocationID', 'DOLocationID'], axis=1)).equals(rows_lon_lat_exist.copy().drop(['PULocationID', 'DOLocationID'], axis=1))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325529ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_polygons = gpd.read_file('./taxi_zones/taxi_zones.shp')\n",
    "gdf_polygons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_polygons.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289df2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gdf_get_location(df):\n",
    "    \"\"\"\n",
    "    Returns a GeoDataFrame with the centroid coordinates (latitude and longitude) \n",
    "    for each pickup and dropoff location in the input GeoDataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (GeoDataFrame): a GeoDataFrame with polygon geometries representing \n",
    "        pickup and dropoff zones.\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame: a GeoDataFrame with the same columns as the input GeoDataFrame, \n",
    "        plus four additional columns: 'pickup_lon', 'pickup_lat', 'dropoff_lon', \n",
    "        and 'dropoff_lat', which represent the centroid coordinates for each pickup \n",
    "        and dropoff location in decimal degrees (WGS84).\n",
    "\n",
    "    \"\"\"\n",
    "    df = gdf_polygons.to_crs(4326)\n",
    "    df['pickup_lon'] = df['geometry'].centroid.x\n",
    "    df['pickup_lat'] = df['geometry'].centroid.y\n",
    "    df['dropoff_lon'] = df['geometry'].centroid.x\n",
    "    df['dropoff_lat'] = df['geometry'].centroid.y\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e6f5b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "gdf_polygons = gdf_get_location(gdf_polygons)\n",
    "gdf_polygons.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b9933",
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_polygons.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c547023",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_geo_df(geo_df, df):  \n",
    "    \"\"\"\n",
    "    Merge the missing longitude and latitude values in `df` with the corresponding values in `geo_df`\n",
    "    using the location IDs in the 'PULocationID' and 'DOLocationID' columns.\n",
    "\n",
    "    Args:\n",
    "        geo_df (pandas.DataFrame): A dataframe containing the mapping between location IDs and their\n",
    "                                   corresponding longitude and latitude values.\n",
    "        df (pandas.DataFrame): A dataframe containing the trip data with missing longitude and latitude values.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A merged dataframe containing the original trip data from `df` with the missing\n",
    "                           longitude and latitude values filled in using the corresponding values from `geo_df`.\n",
    "    \"\"\"\n",
    "    # Merge nan_rows with geo_df to get the missing longtitude/latitude values\n",
    "    # We use 'inner' merge to filter the rows with invalid location ID\n",
    "    merged_nan_rows_PU = pd.merge(df, geo_df[['LocationID', 'pickup_lon', 'pickup_lat']],\n",
    "                           left_on='PULocationID', right_on='LocationID', how='inner')\n",
    "    merged_nan_rows = pd.merge(merged_nan_rows_PU, geo_df[['LocationID', 'dropoff_lon', 'dropoff_lat']],\n",
    "                           left_on='DOLocationID', right_on='LocationID', how='inner')  \n",
    "    return merged_nan_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7198e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_rows_lon_lat = merge_geo_df(gdf_polygons, nan_rows_lon_lat)\n",
    "nan_rows_lon_lat['pickup_longitude'] = nan_rows_lon_lat['pickup_lon']\n",
    "nan_rows_lon_lat['pickup_latitude'] = nan_rows_lon_lat['pickup_lat']\n",
    "nan_rows_lon_lat['dropoff_longitude'] = nan_rows_lon_lat['dropoff_lon']\n",
    "nan_rows_lon_lat['dropoff_latitude'] = nan_rows_lon_lat['dropoff_lat']\n",
    "nan_rows_lon_lat = nan_rows_lon_lat.drop(columns=['pickup_lon', 'pickup_lat', 'dropoff_lon', 'dropoff_lat',\\\n",
    "                                                  'LocationID_x', 'LocationID_y'])\n",
    "nan_rows_lon_lat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d40a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi_ride_sample = pd.concat([nan_rows_lon_lat, rest_rows])\n",
    "yellow_taxi_ride_sample = yellow_taxi_ride_sample.drop(columns=['PULocationID', 'DOLocationID'])\n",
    "yellow_taxi_ride_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83180309",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi_ride_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919a161",
   "metadata": {},
   "source": [
    "### Data Preprocessing - Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f12b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_rides_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a379c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_rides_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa59dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check missing values\n",
    "uber_rides_sample.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f29d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_rides_sample['pickup_datetime'] = pd.to_datetime(uber_rides_sample['pickup_datetime'])\n",
    "uber_rides_sample = uber_rides_sample.drop('key', axis=1) #Since Column 'key' has same values as Column 'pickup_datetime'\n",
    "uber_rides_sample = uber_rides_sample.dropna() #Since there are very less missing values, we simply drop them\n",
    "uber_rides_sample.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2f81ab",
   "metadata": {},
   "source": [
    "### For Uber and Yellow Taxi data, \n",
    "#### remove out of region records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4bddea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_out_region(df):\n",
    "    \"\"\"\n",
    "    Remove rows from `df` that have pickup or dropoff locations outside the specified region of New York City.\n",
    "\n",
    "    The region is defined by the latitude and longitude boundaries:\n",
    "    - Minimum latitude: 40.560445\n",
    "    - Maximum latitude: 40.908524\n",
    "    - Minimum longitude: -74.242330\n",
    "    - Maximum longitude: -73.71704\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): A dataframe containing the trip data with latitude and longitude values.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A filtered dataframe containing the trip data with only the rows that have pickup\n",
    "                           and dropoff locations within the specified region of New York City.\n",
    "    \"\"\"\n",
    "    df = df[df['pickup_latitude'] >= 40.560445]\n",
    "    df = df[df['pickup_latitude'] <= 40.908524]\n",
    "    df = df[df['dropoff_latitude'] >= 40.560445]\n",
    "    df = df[df['dropoff_latitude'] <= 40.908524]\n",
    "    \n",
    "    df = df[df['pickup_longitude'] >= -74.242330]\n",
    "    df = df[df['pickup_longitude'] <= -73.71704]\n",
    "    df = df[df['dropoff_longitude'] >= -74.242330]\n",
    "    df = df[df['dropoff_longitude'] <= -73.71704]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f0d573",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi_ride_sample = remove_out_region(yellow_taxi_ride_sample)\n",
    "uber_rides_sample = remove_out_region(remove_out_region(uber_rides_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc25511a",
   "metadata": {},
   "source": [
    "####  Calculate trip distance between the pickup and dropoff location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7907449",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trip_distance(row):\n",
    "    \"\"\"\n",
    "    Calculate the Euclidean distance (in kilometers) between the pickup location and the dropoff location of a trip.\n",
    "\n",
    "    The function uses the latitude and longitude coordinates of the pickup and dropoff locations to calculate\n",
    "    the distance between them. It assumes that the Earth is a perfect sphere with a radius of 6,371 kilometers.\n",
    "\n",
    "    Args:\n",
    "        row (pandas.Series): A row of a dataframe containing the latitude and longitude values of the pickup\n",
    "                             and dropoff locations of a trip.\n",
    "\n",
    "    Returns:\n",
    "        float: The distance (in kilometers) between the pickup location and the dropoff location of the trip.\n",
    "    \"\"\"\n",
    "    pickup_latitude = row['pickup_latitude']\n",
    "    pickup_longitude = row['pickup_longitude']\n",
    "    dropoff_latitude = row['dropoff_latitude']\n",
    "    dropoff_longitude = row['dropoff_longitude']\n",
    "    \n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [pickup_latitude, pickup_longitude,  dropoff_latitude, dropoff_longitude])\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    dist = math.sqrt(dlat**2 + dlon**2)\n",
    "    \n",
    "    R = 6371\n",
    "    distance = dist * R\n",
    "    return distance\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd12f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi_ride_sample['distance_PD'] = yellow_taxi_ride_sample.apply(calculate_trip_distance, axis=1)\n",
    "uber_rides_sample['distance_PD'] = uber_rides_sample.apply(calculate_trip_distance, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa032c1c",
   "metadata": {},
   "source": [
    "#### Filter out zero distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a9f9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_zero_distance(df):\n",
    "    \"\"\"\n",
    "    Remove trips from a dataframe where the distance between the pickup and dropoff locations is zero.\n",
    "\n",
    "    The function takes a dataframe containing information about taxi trips, including the distance between\n",
    "    the pickup and dropoff locations of each trip. It removes all rows where the distance is zero, as these\n",
    "    trips are likely to be erroneous or incomplete.\n",
    "\n",
    "    Args:\n",
    "        df (pandas.DataFrame): A dataframe containing information about taxi trips, including the distance\n",
    "                               between the pickup and dropoff locations of each trip.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A dataframe with the same columns as the input dataframe, but with all rows where\n",
    "                           the distance between the pickup and dropoff locations is zero removed.\n",
    "    \"\"\"\n",
    "    condition = df['distance_PD'] == 0\n",
    "    df = df.drop(df[condition].index)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac377ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi_ride_sample = remove_zero_distance(yellow_taxi_ride_sample)\n",
    "uber_rides_sample = remove_zero_distance(uber_rides_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b80c4b",
   "metadata": {},
   "source": [
    "### Cleaned Yellow Taxi Data sample and Uber Data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea9dfba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "yellow_taxi_ride_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0a8c7c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "uber_rides_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6649320a",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51667483",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './weather/'\n",
    "all_files = glob.glob(path + \"*.csv\")\n",
    "data_frames = []\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename)\n",
    "    data_frames.append(df)\n",
    "weather_data = pd.concat(data_frames, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809582f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb457592",
   "metadata": {},
   "source": [
    "### Hourly Weather Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d51cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_hourly = weather_data.iloc[:, 1:4].join(weather_data[['HourlyPrecipitation', 'HourlyWindSpeed']])\n",
    "weather_data_hourly.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb2ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_hourly.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1ef41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check missing values\n",
    "weather_data_hourly.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61db2313",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Unique values in Latitude: \",weather_data_hourly['LATITUDE'].unique())\n",
    "print(\"Unique values in Longtitude: \",weather_data_hourly['LONGITUDE'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1076b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since there is only one unique longtitude and latitude, which means that all the weather data are from the same place\n",
    "#We drop the two columns \"LATITUDE\", \"LONGTITUDE\"\n",
    "\n",
    "weather_data_hourly = weather_data_hourly.drop(columns=['LATITUDE', 'LONGITUDE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21a6768",
   "metadata": {},
   "source": [
    "#### Fill in missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001b2fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fills missing values in 'HourlyPrecipitation' with 0\n",
    "weather_data_hourly['HourlyPrecipitation'].fillna('0.0', inplace=True)\n",
    "\n",
    "# Fills missing values in 'HourlyWindSpeed' with the last known value in the same column\n",
    "weather_data_hourly['HourlyWindSpeed'] = weather_data_hourly['HourlyWindSpeed'].fillna(method='ffill')\n",
    "\n",
    "#Change 'DATE' column to Datetime type\n",
    "weather_data_hourly['DATE'] = pd.to_datetime(weather_data_hourly['DATE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b88ade9",
   "metadata": {},
   "source": [
    "#### In `HourlyPrecipitation` change all value `T`  to be `0.00001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a057b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_hourly['HourlyPrecipitation'] = weather_data_hourly['HourlyPrecipitation'].str.replace('s', '')\n",
    "weather_data_hourly['HourlyPrecipitation'] = weather_data_hourly['HourlyPrecipitation'].replace('T', '0.00001')\n",
    "weather_data_hourly['HourlyPrecipitation'] = weather_data_hourly['HourlyPrecipitation'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba714064",
   "metadata": {},
   "source": [
    "### Cleaned Hourly Weather Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543a9cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28d24a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_hourly.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4716a66",
   "metadata": {},
   "source": [
    "### Daily Weather Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_daily = weather_data[['DATE','Sunrise','Sunset','DailyAverageWindSpeed','DailyPeakWindSpeed',\n",
    "                                   'DailySustainedWindSpeed','DailyPrecipitation']]\n",
    "weather_data_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a190cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_daily.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5431f0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Group by 'DATE' (daily)\n",
    "date_format = '%Y-%m-%d'\n",
    "weather_data_daily['DATE'] = pd.to_datetime(weather_data_daily['DATE'])\n",
    "weather_data_daily['DATE'] = pd.to_datetime(weather_data_daily['DATE'], format=date_format).dt.date\n",
    "weather_data_daily['DATE'] = pd.to_datetime(weather_data_daily['DATE'])\n",
    "\n",
    "def filter_rows(group):\n",
    "    \"\"\"\n",
    "    Filter rows of a group based on missing values.\n",
    "\n",
    "    The function takes a group of rows from a larger dataframe and filters out any rows where all the columns\n",
    "    except the first one are missing (i.e., contain NaN values). If all the rows in the group have this property,\n",
    "    the function returns the first row of the group.\n",
    "\n",
    "    Args:\n",
    "        group (pandas.DataFrame): A dataframe representing a group of rows from a larger dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A dataframe with the same columns as the input dataframe, but with any rows where\n",
    "                           all the columns except the first one are missing removed. If all the rows in the\n",
    "                           group have this property, the function returns the first row of the group.\n",
    "    \"\"\"\n",
    "    is_all_nan = group.iloc[:, 1:].isna().all(axis=1)\n",
    "    if is_all_nan.all():\n",
    "        return group.head(1)\n",
    "    return group.dropna(subset=group.columns[1:], how='all')\n",
    "\n",
    "date_group = weather_data_daily.groupby('DATE')\n",
    "weather_data_daily = date_group.apply(filter_rows)\n",
    "weather_data_daily['DATE'] = weather_data_daily['DATE'].dt.strftime('%Y-%m-%d')\n",
    "weather_data_daily = weather_data_daily.reset_index(drop=True)\n",
    "weather_data_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680b4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_daily.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23752149",
   "metadata": {},
   "source": [
    "#### Fill in the missing values according the Hourly Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7b070b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weather_data_hourly_copy = weather_data_hourly.copy()\n",
    "weather_data_hourly_copy['DATE'] = pd.to_datetime(weather_data_hourly_copy['DATE'], format=date_format).dt.date\n",
    "weather_data_hourly_copy['DATE'] = pd.to_datetime(weather_data_hourly_copy['DATE'])\n",
    "\n",
    "daily_avg_windspeed = weather_data_hourly_copy.groupby('DATE')['HourlyWindSpeed'].mean()\n",
    "daily_peak_windspeed = weather_data_hourly_copy.groupby('DATE')['HourlyWindSpeed'].max()\n",
    "daily_sustained_windspeed = weather_data_hourly_copy.groupby('DATE')['HourlyWindSpeed'].apply(lambda x: x.value_counts().idxmax())\n",
    "daily_precipitation = weather_data_hourly_copy.groupby('DATE')['HourlyPrecipitation'].sum()\n",
    "\n",
    "weather_data_daily['DailyAverageWindSpeed'] = weather_data_daily.apply(lambda row: \n",
    "                                                row['DailyAverageWindSpeed'] \n",
    "                                                if pd.notna(row['DailyAverageWindSpeed']) \n",
    "                                                else daily_avg_windspeed[row['DATE']], \n",
    "                                                axis=1)\n",
    "weather_data_daily['DailyPeakWindSpeed'] = weather_data_daily.apply(lambda row: \n",
    "                                                row['DailyPeakWindSpeed'] \n",
    "                                                if pd.notna(row['DailyPeakWindSpeed']) \n",
    "                                                else daily_peak_windspeed[row['DATE']], \n",
    "                                                axis=1)\n",
    "weather_data_daily['DailySustainedWindSpeed'] = weather_data_daily.apply(lambda row: \n",
    "                                                row['DailySustainedWindSpeed'] \n",
    "                                                if pd.notna(row['DailySustainedWindSpeed']) \n",
    "                                                else daily_sustained_windspeed[row['DATE']], \n",
    "                                                axis=1)\n",
    "weather_data_daily['DailyPrecipitation'] = weather_data_daily.apply(lambda row: \n",
    "                                                row['DailyPrecipitation'] \n",
    "                                                if pd.notna(row['DailyPrecipitation']) \n",
    "                                                else daily_precipitation[row['DATE']], \n",
    "                                                axis=1)\n",
    "\n",
    "weather_data_daily['DATE'] = pd.to_datetime(weather_data_daily['DATE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7fa43f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_daily.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fcbd7e",
   "metadata": {},
   "source": [
    "#### Change all value `T` in `DailyPrecipitation` to be `0.00001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ecb35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_daily['DailyPrecipitation'] = weather_data_daily['DailyPrecipitation'].replace('T', '0.00001')\n",
    "weather_data_daily['DailyPrecipitation'] = weather_data_daily['DailyPrecipitation'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ed5eb",
   "metadata": {},
   "source": [
    "### Cleaned Daily Weather Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9391ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunrise_sunset = weather_data_daily.copy().dropna()\n",
    "sunrise_sunset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunrise_sunset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e5c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_hourly.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa15ee3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather_data_daily = weather_data_daily.drop(columns=['Sunrise','Sunset'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681524e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data_daily.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa0051c",
   "metadata": {},
   "source": [
    "# Part 2: Storing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1736c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///project.db', echo=True)\n",
    "\n",
    "# COMMAND = [\"\"\"\n",
    "# DROP TABLE IF EXISTS yellow_taxi_ride\"\"\",\n",
    "# \"\"\"\n",
    "# DROP TABLE IF EXISTS hourly_weather_data\"\"\",\n",
    "# \"\"\"\n",
    "# DROP TABLE IF EXISTS daily_weather_data\"\"\",\n",
    "# \"\"\"\n",
    "# DROP TABLE IF EXISTS uber_rides\"\"\",\n",
    "# \"\"\"\n",
    "# DROP TABLE IF EXISTS sunrise_sunset_data\"\"\"\n",
    "# ]\n",
    "\n",
    "# for i in COMMAND:\n",
    "#     engine.execute(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968c7474",
   "metadata": {},
   "source": [
    "### Five tables (Including Sunrise_Sunset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad08d929",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = sqlite3.connect(\"project.db\")\n",
    "\n",
    "#Yellow Taxi trips\n",
    "with connection:\n",
    "    connection.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS yellow_taxi_ride (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            trip_distance REAL,\n",
    "            passenger_count INTEGER,\n",
    "            total_amount REAL,\n",
    "            dropoff_datetime DATETIME,\n",
    "            pickup_longitude REAL,\n",
    "            pickup_latitude REAL,\n",
    "            dropoff_latitude REAL,\n",
    "            dropoff_longitude REAL,\n",
    "            pickup_datetime DATETIME,\n",
    "            distance_PD REAL,\n",
    "            tip_amount REAL\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "#Uber trips\n",
    "with connection:\n",
    "    connection.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS uber_rides (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            fare_amount REAL,\n",
    "            pickup_datetime DATETIME,\n",
    "            pickup_longitude REAL,\n",
    "            pickup_latitude REAL,\n",
    "            dropoff_longitude REAL,\n",
    "            dropoff_latitude REAL,\n",
    "            passenger_count INTEGER,\n",
    "            distance_PD REAL\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "#Hourly Weather\n",
    "with connection:\n",
    "    connection.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS hourly_weather_data (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            DATE DATETIME,\n",
    "            HourlyPrecipitation REAL,\n",
    "            HourlyWindSpeed REAL\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "#Daily Weather\n",
    "with connection:\n",
    "    connection.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS daily_weather_data (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            DATE DATETIME,\n",
    "            DailyAverageWindSpeed REAL,\n",
    "            DailyPeakWindSpeed REAL,\n",
    "            DailySustainedWindSpeed REAL,\n",
    "            DailyPrecipitation REAL\n",
    "        );\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "# Sunrise Sunset\n",
    "with connection:\n",
    "    connection.execute(\n",
    "        \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS sunrise_sunset_data (\n",
    "            id INTEGER PRIMARY KEY,\n",
    "            DATE DATETIME,\n",
    "            Sunrise  REAL,\n",
    "            Sunset REAL,\n",
    "            DailyAverageWindSpeed REAL,\n",
    "            DailyPeakWindSpeed REAL，\n",
    "            DailySustainedWindSpeed REAL,\n",
    "            DailyPrecipitation REAL\n",
    "        );\n",
    "        \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4cd8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi_df = yellow_taxi_ride_sample.copy()\n",
    "yellow_taxi_df.reset_index(inplace=True)\n",
    "yellow_taxi_df.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "uber_df = uber_rides_sample.copy()\n",
    "uber_df.reset_index(inplace=True)\n",
    "uber_df.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "hourly_weather_df = weather_data_hourly.copy()\n",
    "hourly_weather_df.reset_index(inplace=True)\n",
    "hourly_weather_df.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "daily_weather_df = weather_data_daily.copy()\n",
    "daily_weather_df.reset_index(inplace=True)\n",
    "daily_weather_df.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "sunrise_set_df = sunrise_sunset.copy()\n",
    "sunrise_set_df.reset_index(inplace=True)\n",
    "sunrise_set_df.rename(columns={'index': 'id'}, inplace=True)\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    yellow_taxi_df.to_sql('yellow_taxi_ride', conn, if_exists='replace', index=False)\n",
    "    uber_df.to_sql('uber_rides', conn, if_exists='replace', index=False)\n",
    "    hourly_weather_df.to_sql('hourly_weather_data', conn, if_exists='replace', index=False)\n",
    "    daily_weather_df.to_sql('daily_weather_data', conn, if_exists='replace', index=False)\n",
    "    sunrise_set_df.to_sql('sunrise_sunset_data', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45809a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('project.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT name, sql FROM sqlite_master WHERE type='table'\")\n",
    "tables = cur.fetchall()\n",
    "\n",
    "with open('schema.sql', 'w') as f:\n",
    "    for table in tables:\n",
    "        f.write(f'{table[1]};\\n')\n",
    "        \n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09300456",
   "metadata": {},
   "source": [
    "# Part 3: Understanding Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098efaf0",
   "metadata": {},
   "source": [
    "## Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5e92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"\"\"\n",
    "SELECT \n",
    "    strftime('%H', pickup_datetime) AS hour_of_day,\n",
    "    COUNT(*) AS ride_count\n",
    "FROM \n",
    "    yellow_taxi_ride \n",
    "WHERE \n",
    "    pickup_datetime BETWEEN '2009-01-01' AND '2015-06-30'\n",
    "GROUP BY \n",
    "    hour_of_day \n",
    "ORDER BY \n",
    "    ride_count DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efaa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"popularity_yellow_taxi_01_2009_06-2015.sql\", \"w\") as f:\n",
    "    f.write(query1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa65656f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('project.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute(query1)\n",
    "results1 = cur.fetchall()\n",
    "\n",
    "for row in results1:\n",
    "    print(row)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2e041",
   "metadata": {},
   "source": [
    "## Query 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc949dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query2 = \"\"\"\n",
    "SELECT \n",
    "    strftime('%w', pickup_datetime) AS day_of_week,\n",
    "    COUNT(*) AS ride_count\n",
    "FROM \n",
    "    uber_rides\n",
    "WHERE \n",
    "    pickup_datetime BETWEEN '2009-01-01' AND '2015-06-30'\n",
    "GROUP BY \n",
    "    day_of_week \n",
    "ORDER BY \n",
    "    ride_count DESC;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70b2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"popularity_Uber_rides_week_days.sql\", \"w\") as f:\n",
    "    f.write(query2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bcd1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('project.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute(query2)\n",
    "results2 = cur.fetchall()\n",
    "\n",
    "for row in results2:\n",
    "    print(row)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7aafeb",
   "metadata": {},
   "source": [
    "## Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ecf25e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query3 = \"\"\"\n",
    "SELECT CAST((SELECT distance_PD\n",
    "             FROM (\n",
    "               SELECT distance_PD, NTILE(100) OVER (ORDER BY distance_PD) AS percentile\n",
    "               FROM (\n",
    "                 SELECT distance_PD FROM yellow_taxi_ride WHERE pickup_datetime BETWEEN '2013-07-01' AND '2013-07-31'\n",
    "                 UNION ALL\n",
    "                 SELECT distance_PD FROM uber_rides WHERE pickup_datetime BETWEEN '2013-07-01' AND '2013-07-31'\n",
    "               ) AS hired_trips\n",
    "             )\n",
    "             WHERE percentile = 95\n",
    "             LIMIT 1) AS REAL) AS percentile_95;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7b2e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hired_trips_distance_95_percentile_07_2013.sql\", \"w\") as f:\n",
    "    f.write(query3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513687f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('project.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute(query3)\n",
    "results3 = cur.fetchall()\n",
    "\n",
    "for row in results3:\n",
    "    print(row)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca9c39",
   "metadata": {},
   "source": [
    "## Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78359e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query4 = \"\"\"\n",
    "SELECT\n",
    "    date(date_time) AS date,\n",
    "    SUM(num_rides) AS num_rides,\n",
    "    AVG(avg_distance) AS avg_distance\n",
    "FROM\n",
    "    (\n",
    "    SELECT\n",
    "        dropoff_datetime AS date_time,\n",
    "        1 AS num_rides,\n",
    "        trip_distance AS avg_distance\n",
    "    FROM\n",
    "        yellow_taxi_ride\n",
    "    WHERE\n",
    "        strftime('%Y-%m', dropoff_datetime) BETWEEN '2009-01' AND '2009-06'\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT\n",
    "        pickup_datetime AS date_time,\n",
    "        1 AS num_rides,\n",
    "        distance_PD AS avg_distance\n",
    "    FROM\n",
    "        uber_rides\n",
    "    WHERE\n",
    "        strftime('%Y-%m', pickup_datetime) BETWEEN '2009-01' AND '2009-06'\n",
    "    ) AS hired_rides\n",
    "GROUP BY\n",
    "    date\n",
    "ORDER BY\n",
    "    num_rides DESC\n",
    "LIMIT\n",
    "    10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544f4906",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"2009_top_10_hired_rides_average_distance.sql\", \"w\") as f:\n",
    "    f.write(query4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0471b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('project.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute(query4)\n",
    "results4 = cur.fetchall()\n",
    "\n",
    "for row in results4:\n",
    "    print(row)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd69745",
   "metadata": {},
   "source": [
    "## Query 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c47ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "query5 = \"\"\"\n",
    "SELECT date, avg_wind_speed, SUM(hired_trips + total_rides) AS total_trips\n",
    "FROM (\n",
    "    SELECT d.date, d.DailyAverageWindSpeed AS avg_wind_speed, COUNT(y.id) AS hired_trips, 0 AS total_rides\n",
    "    FROM daily_weather_data d\n",
    "    JOIN uber_rides y ON DATE(d.date) = DATE(y.pickup_datetime)\n",
    "    WHERE strftime('%Y', d.date) = '2014'\n",
    "    GROUP BY d.date\n",
    "\n",
    "    UNION ALL\n",
    "\n",
    "    SELECT d.date, d.DailyAverageWindSpeed AS avg_wind_speed, 0 AS hired_trips, COUNT(y.id) AS total_rides\n",
    "    FROM daily_weather_data d\n",
    "    JOIN yellow_taxi_ride y ON DATE(d.date) = DATE(y.pickup_datetime)\n",
    "    WHERE strftime('%Y', d.date) = '2014'\n",
    "    GROUP BY d.date\n",
    ")\n",
    "GROUP BY date\n",
    "ORDER BY avg_wind_speed DESC\n",
    "LIMIT 10;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b45cc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"windest_10_days_2014_hired_trips.sql\", \"w\") as f:\n",
    "    f.write(query5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b9323",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('project.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute(query5)\n",
    "results5 = cur.fetchall()\n",
    "\n",
    "for row in results5:\n",
    "    print(row)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69554612",
   "metadata": {},
   "source": [
    "## Query 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55580462",
   "metadata": {},
   "outputs": [],
   "source": [
    "query6 = \"\"\"\n",
    "WITH total_hired_trips AS \n",
    "(\n",
    "    SELECT strftime('%Y-%m-%d %H:00:00', pickup_datetime) AS date_hour_time FROM uber_rides\n",
    "    WHERE strftime('%Y-%m-%d', pickup_datetime) BETWEEN '2012-10-22' AND '2012-11-07'\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT strftime('%Y-%m-%d %H:00:00', pickup_datetime) AS date_hour_time FROM yellow_taxi_ride\n",
    "    WHERE strftime('%Y-%m-%d', pickup_datetime) BETWEEN '2012-10-22' AND '2012-11-07'\n",
    ")\n",
    "\n",
    "SELECT strftime('%Y-%m-%d %H:00:00', hourly_weather_data.DATE) AS weather_date_hour, COUNT(*) AS number_hired_rides, SUM(HourlyPrecipitation) AS total_precipitation, AVG(HourlyWindSpeed) AS average_wind_speed\n",
    "FROM hourly_weather_data\n",
    "JOIN total_hired_trips ON weather_date_hour = total_hired_trips.date_hour_time\n",
    "WHERE strftime('%Y-%m-%d', hourly_weather_data.DATE) BETWEEN '2012-10-22' AND '2012-11-07'\n",
    "GROUP BY weather_date_hour\n",
    "ORDER BY weather_date_hour\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3530518",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hurricane_trips_precipitation_wind_speed_hourly.sql\", \"w\") as f:\n",
    "    f.write(query6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02b9d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('project.db')\n",
    "cur = conn.cursor()\n",
    "cur.execute(query6)\n",
    "results6 = cur.fetchall()\n",
    "\n",
    "for row in results6:\n",
    "    print(row)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df44523",
   "metadata": {},
   "source": [
    "# Part 4: Visualizing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef00d75",
   "metadata": {},
   "source": [
    "### Plot 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db152ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [int(row[0]) for row in results1]\n",
    "y = [row[1] for row in results1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x, y)\n",
    "\n",
    "ax.set_xlabel('Hour of the day')\n",
    "ax.set_ylabel('Number of rides')\n",
    "ax.set_title('Popularity of Yellow Taxi rides (2009-01 to 2015-06)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf6c565",
   "metadata": {},
   "source": [
    "### Plot 2: the average distance traveled per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0217f570",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi_rides_distance = yellow_taxi_ride_sample[['pickup_datetime','distance_PD']]\n",
    "uber_rides_distance = uber_rides_sample[['pickup_datetime','distance_PD']]\n",
    "uber_rides_distance.loc[:, 'pickup_datetime'] = uber_rides_distance.loc[:, 'pickup_datetime'].dt.tz_localize(None)\n",
    "rides_distance = pd.concat([yellow_taxi_rides_distance, uber_rides_distance], ignore_index=True)\n",
    "rides_distance.sort_values('pickup_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dbd771",
   "metadata": {},
   "outputs": [],
   "source": [
    "rides_distance['pickup_datetime'] = pd.to_datetime(rides_distance['pickup_datetime'])\n",
    "rides_distance['pickup_datetime'] = rides_distance['pickup_datetime'].apply(lambda x: x.strftime('%m'))\n",
    "\n",
    "# Group by month and calculate mean and standard error of mean (sem)\n",
    "rides_distance_monthly = rides_distance.groupby('pickup_datetime')['distance_PD'].agg(['mean', 'sem'])\n",
    "\n",
    "# Calculate 90% confidence interval\n",
    "rides_distance_monthly['lower'] = rides_distance_monthly['mean'] - 1.645*rides_distance_monthly['sem']\n",
    "rides_distance_monthly['upper'] = rides_distance_monthly['mean'] + 1.645*rides_distance_monthly['sem']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,6))\n",
    "ax.errorbar(x=rides_distance_monthly.index, y=rides_distance_monthly['mean'], yerr=rides_distance_monthly['sem'], label='Mean distance', fmt='o')\n",
    "ax.fill_between(rides_distance_monthly.index, rides_distance_monthly['lower'], rides_distance_monthly['upper'], alpha=0.2, label='90% CI')\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Distance')\n",
    "ax.set_title('Average distance traveled per month (2009 to 2015)')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36bb1a1",
   "metadata": {},
   "source": [
    "### Plot 3: drop offs at three major New York airports: LGA, JFK, and EWR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e68ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi_dropoffs = yellow_taxi_ride_sample[['pickup_datetime','dropoff_longitude','dropoff_latitude']]\n",
    "uber_dropoffs = uber_rides_sample[['pickup_datetime','dropoff_longitude','dropoff_latitude']]\n",
    "uber_dropoffs.loc[:, 'pickup_datetime'] = uber_dropoffs.loc[:, 'pickup_datetime'].dt.tz_localize(None)\n",
    "rides_dropoffs = pd.concat([yellow_taxi_dropoffs, uber_dropoffs], ignore_index=True)\n",
    "rides_dropoffs.sort_values('pickup_datetime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d0a035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [latitude, longtitude] coordinates\n",
    "LGA_coords = [40.7769, -73.8740]\n",
    "JFK_coords = [40.6413, -73.7781]\n",
    "EWR_coords = [40.6895, -74.1745]\n",
    "\n",
    "# Load the dataset and filter by drop offs at each airport\n",
    "LGA_df = rides_dropoffs[(rides_dropoffs.dropoff_latitude >= LGA_coords[0]-0.02) & (rides_dropoffs.dropoff_latitude <= LGA_coords[0]+0.02) &\n",
    "                        (rides_dropoffs.dropoff_longitude >= LGA_coords[1]-0.02) & (rides_dropoffs.dropoff_longitude <= LGA_coords[1]+0.02)]\n",
    "JFK_df = rides_dropoffs[(rides_dropoffs.dropoff_latitude >= JFK_coords[0]-0.02) & (rides_dropoffs.dropoff_latitude <= JFK_coords[0]+0.02) &\n",
    "                        (rides_dropoffs.dropoff_longitude >= JFK_coords[1]-0.02) & (rides_dropoffs.dropoff_longitude <= JFK_coords[1]+0.02)]\n",
    "EWR_df = rides_dropoffs[(rides_dropoffs.dropoff_latitude >= EWR_coords[0]-0.02) & (rides_dropoffs.dropoff_latitude <= EWR_coords[0]+0.02) &\n",
    "                        (rides_dropoffs.dropoff_longitude >= EWR_coords[1]-0.02) & (rides_dropoffs.dropoff_longitude <= EWR_coords[1]+0.02)]\n",
    "\n",
    "# Group the data by day of the week and count the number of drop offs\n",
    "LGA_day_counts = LGA_df.groupby(LGA_df['pickup_datetime'].dt.dayofweek).size()\n",
    "JFK_day_counts = JFK_df.groupby(JFK_df['pickup_datetime'].dt.dayofweek).size()\n",
    "EWR_day_counts = EWR_df.groupby(EWR_df['pickup_datetime'].dt.dayofweek).size()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,6))\n",
    "bar_width = 0.25\n",
    "ax.bar(LGA_day_counts.index - bar_width, LGA_day_counts.values, width=bar_width, label='LGA')\n",
    "ax.bar(JFK_day_counts.index, JFK_day_counts.values, width=bar_width, label='JFK')\n",
    "ax.bar(EWR_day_counts.index + bar_width, EWR_day_counts.values, width=bar_width, label='EWR')\n",
    "ax.set_xticks(range(7))\n",
    "ax.set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])\n",
    "ax.set_xlabel('Day of the week')\n",
    "ax.set_ylabel('Number of drop offs')\n",
    "ax.set_title('Drop offs at airports(LGA, JFK, EWR) vs. Day of Week (2009 to 2015)')\n",
    "ax.legend()\n",
    "ax.grid(axis=\"y\",linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a9452",
   "metadata": {},
   "source": [
    "### Plot 4: heatmap of all hired trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40832c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi_rides_hired = yellow_taxi_ride_sample[['pickup_longitude','pickup_latitude']]\n",
    "uber_rides_hired = uber_rides_sample[['pickup_longitude','pickup_latitude']]\n",
    "rides_hired = pd.concat([yellow_taxi_rides_hired, uber_rides_hired], ignore_index=True)\n",
    "rides_hired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap = folium.Map(location=[rides_hired['pickup_latitude'].mean(), rides_hired['pickup_longitude'].mean()], zoom_start=10)\n",
    "title_html = '''\n",
    "             <h3 align=\"center\" style=\"font-size:20px\"><b>Hired Trips Heatmap</b></h3>\n",
    "             '''\n",
    "heatmap.get_root().html.add_child(folium.Element(title_html))\n",
    "heatmap.add_child(folium.plugins.HeatMap(rides_hired[['pickup_latitude', 'pickup_longitude']].values.tolist(), name='Heatmap', control=False))\n",
    "folium.LayerControl().add_to(heatmap)\n",
    "heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9757e0",
   "metadata": {},
   "source": [
    "### Plot 5: `tip amount` vs. `distance` for Yellow Taxi rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2b470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_distance = yellow_taxi_ride_sample[(yellow_taxi_ride_sample[\"tip_amount\"] >= 0) & \n",
    "                                       (yellow_taxi_ride_sample[\"tip_amount\"] < 30) & \n",
    "                                       (yellow_taxi_ride_sample[\"distance_PD\"] >= 0) & \n",
    "                                       (yellow_taxi_ride_sample[\"distance_PD\"] <= 35)]\n",
    "\n",
    "plt.scatter(tip_distance[\"distance_PD\"], tip_distance[\"tip_amount\"])\n",
    "plt.xlabel(\"Straight-line Distance\")\n",
    "plt.ylabel(\"Tip Amount\")\n",
    "plt.title(\"Tip Amount vs Straight-line Distance for Yellow Taxi Rides\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6acb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_distance = yellow_taxi_ride_sample[(yellow_taxi_ride_sample[\"tip_amount\"] >= 0) & \n",
    "                                       (yellow_taxi_ride_sample[\"tip_amount\"] < 30) & \n",
    "                                       (yellow_taxi_ride_sample[\"trip_distance\"] >= 0) & \n",
    "                                       (yellow_taxi_ride_sample[\"trip_distance\"] <= 35)]\n",
    "\n",
    "plt.scatter(tip_distance[\"trip_distance\"], tip_distance[\"tip_amount\"])\n",
    "plt.xlabel(\"Actual Distance\")\n",
    "plt.ylabel(\"Tip Amount\")\n",
    "plt.title(\"Tip Amount vs Actual Distance for Yellow Taxi Rides\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0211a7d2",
   "metadata": {},
   "source": [
    "### Plot 6: Daily `tip amount` vs. `precipitation`  for Yellow Taxi rides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd4a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "yellow_taxi_ride_daily = yellow_taxi_ride_sample.copy()\n",
    "yellow_taxi_ride_daily['DATE'] = pd.to_datetime(yellow_taxi_ride_daily['pickup_datetime'], format=date_format).dt.date\n",
    "yellow_taxi_ride_daily['DATE'] = pd.to_datetime(yellow_taxi_ride_daily['DATE'])\n",
    "\n",
    "daily_tip = yellow_taxi_ride_daily.groupby('DATE')['tip_amount'].sum()\n",
    "tip_weather_daily = pd.merge(weather_data_daily, daily_tip,\n",
    "                           left_on='DATE', right_on=daily_tip.index, how='inner')\n",
    "tip_weather_daily['DailyPrecipitation'] = tip_weather_daily['DailyPrecipitation'].replace('T', 0.0).astype(float)\n",
    "tip_weather_daily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae3ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tip_weather = tip_weather_daily[(tip_weather_daily[\"tip_amount\"] >= 0) & \n",
    "                                       (tip_weather_daily[\"tip_amount\"] < 300) & \n",
    "                                       (tip_weather_daily[\"DailyPrecipitation\"] >= 0) & \n",
    "                                       (tip_weather_daily[\"DailyPrecipitation\"] <= 5)]\n",
    "\n",
    "plt.scatter(tip_weather[\"DailyPrecipitation\"], tip_weather[\"tip_amount\"])\n",
    "plt.xlabel(\"Precipitation Amount\")\n",
    "plt.ylabel(\"Tip Amount\")\n",
    "plt.title(\"Daily Tip Amount vs Precipitation Amount for Yellow Taxi Rides\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a760d",
   "metadata": {},
   "source": [
    "### Plot 7: Animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89178beb",
   "metadata": {},
   "source": [
    "#### Question: How does the DailyAverageWindSpeed vary over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44221b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "def animate(i):\n",
    "    plt.cla()\n",
    "    data = weather_data_daily.iloc[:i+1]\n",
    "    plt.plot(data['DATE'], data['DailyAverageWindSpeed'], color='blue')\n",
    "    plt.xlabel('Date')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Daily Average Wind Speed')\n",
    "    plt.title('Daily Average Wind Speed Over Time')\n",
    "    plt.grid(True)\n",
    "    \n",
    "ani = FuncAnimation(plt.gcf(), animate, frames=len(weather_data_daily), interval=1000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba647a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
